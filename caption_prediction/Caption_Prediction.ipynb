{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8848,"status":"ok","timestamp":1751044074123,"user":{"displayName":"Ivan Mihov","userId":"05068184991104003121"},"user_tz":-180},"id":"oHJ9Y3oTe4kK","outputId":"79c26a89-7eb6-4d35-f529-58ceb1fa8618"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# !fusermount -u /content/drive  # Unmount if already mounted\n","# !rm -rf /content/drive         # Remove any leftover files\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","source":["###Dependancies"],"metadata":{"id":"FlsbRjKEz2AT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8038,"status":"ok","timestamp":1750954115166,"user":{"displayName":"Ivan Mihov","userId":"05068184991104003121"},"user_tz":-180},"id":"P0TrS6_CY0CH","outputId":"3e14e049-b013-4971-b51d-ed03a7502177"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/tylin/coco-caption\n","  Cloning https://github.com/tylin/coco-caption to /tmp/pip-req-build-_h1_r7g8\n","  Running command git clone --filter=blob:none --quiet https://github.com/tylin/coco-caption /tmp/pip-req-build-_h1_r7g8\n","  Resolved https://github.com/tylin/coco-caption to commit 3a9afb2682141a03e1cdc02b0df6770d2c884f6f\n","\u001b[31mERROR: git+https://github.com/tylin/coco-caption does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install torch torchvision transformers\n","!pip install datasets accelerate wandb\n","!pip install opencv-python pillow\n","!pip install scikit-learn nltk rouge-score\n","!pip install bert-score\n","!pip install rouge_score\n","!pip install medcat spacy scispacy\n","\n","\n"]},{"cell_type":"markdown","source":["# Medical Image Captioning with Concept-Aware BLIP Fine-Tuning\n","\n","This notebook fine-tunes the `Salesforce/blip-image-captioning-base` model to generate medical captions using radiology images and concept prompts.\n","\n","##  Dataset Structure\n","- **Images**: JPG files named as `{ID}.jpg`\n","- **Captions**: `train_captions.csv` with fields: `ID`, `Caption`\n","- **Concepts**: `train_concepts.csv` with `ID`, `CUIs`\n","- **Concept Mapping**: `cui_names.csv` maps `CUI` → readable name\n","\n","##  Data Preprocessing\n","- Cleans abbreviations (`CT`, `MRI`, etc.)\n","- Removes noisy/empty/too short/long captions\n","- Builds anatomical & modality prompt vocab\n","- Generates 2 types of prompts:\n","  - **Smart prompt**: based on anatomy + modality\n","  - **Concept-aware prompt**: directly includes top UMLS concepts\n","\n","##  Model: BLIP (Base)\n","- `Salesforce/blip-image-captioning-base` (ViT-GPT2)\n","- Inputs: image + optional prompt\n","- Outputs: caption\n","- Tokenizer: BLIP processor (auto-handled)\n","\n","##  Fine-Tuning Techniques\n","- **Prompt-based fine-tuning**: the prompt is prepended to the caption\n","- **Label masking**: model learns only to predict the caption part (not the prompt)\n","- **Custom scoring**: promotes captions that mention concepts & avoid repetition\n","- **Dynamic prompts**: tries 3 generation types: no prompt, smart prompt, and concept-aware\n","\n","##  Training Details\n","- Optimizer: `AdamW` with `lr=2e-5`, `weight_decay=0.01`\n","- Scheduler: `get_linear_schedule_with_warmup` (10% warmup steps)\n","- Epochs: 5 (adjustable)\n","- Batch Size: 2 (lowered for memory constraints)\n","- Loss: CrossEntropy via `labels=inputs['input_ids']` (with masked prompt tokens)\n","- Gradient clipping: max norm = 1.0\n","- Train/Val/Test split: 80/10/10% from 10,000 samples\n","- Early stopping: model is saved if validation loss improves\n","\n","##  Generation Settings\n","- `num_beams=4`, `max_new_tokens=40`, `repetition_penalty=2.0`\n","- `no_repeat_ngram_size=2`, `length_penalty=0.9`\n","- Optionally adds missing concepts post-generation\n","- Removes repeated words, excessive punctuation\n","\n","##  Output\n","- Trained model saved at: `/content/.../best_medical_blip`\n","- Processor saved alongside the model\n","- Example usage:\n","\n","##Results\n","- BLEU-4 Average Score: 0.0170\n","-ROUGE-L Average Score: 0.1995\n","```python\n","model = BlipForConditionalGeneration.from_pretrained(\"/content/.../best_medical_blip\")\n","processor = BlipProcessor.from_pretrained(\"/content/.../best_medical_blip\")\n"],"metadata":{"id":"SBLWtZBu0yqE"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4850,"status":"ok","timestamp":1750956135019,"user":{"displayName":"Ivan Mihov","userId":"05068184991104003121"},"user_tz":-180},"id":"CMqYG5IpHijt","outputId":"2baff910-5da9-4abe-af0c-44b9a8c337dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.5.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Downloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.4\n"]}],"source":["!pip install evaluate"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    BlipProcessor, BlipForConditionalGeneration,\n","    get_linear_schedule_with_warmup, set_seed\n",")\n","from sklearn.model_selection import train_test_split\n","import json\n","import re\n","from typing import List, Dict, Tuple\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random seeds for reproducibility\n","set_seed(42)\n","\n","class MedicalCaptionDataset(Dataset):\n","    \"\"\"Optimized dataset for medical image captioning\"\"\"\n","\n","    def __init__(self, image_dir: str, caption_csv: str, concept_csv: str,\n","                 mapping_csv: str, processor, split: str = 'train',\n","                 max_caption_length: int = 100):\n","\n","        self.image_dir = image_dir\n","        self.processor = processor\n","        self.max_caption_length = max_caption_length\n","        self.split = split\n","\n","        # Load and merge data\n","        self._load_and_process_data(caption_csv, concept_csv, mapping_csv)\n","\n","        # Medical vocabulary for better prompting\n","        self.modality_terms = {\n","            'radiograph': ['radiograph', 'x-ray', 'xray'],\n","            'ct': ['ct', 'computed tomography', 'cat scan'],\n","            'mri': ['mri', 'magnetic resonance'],\n","            'ultrasound': ['ultrasound', 'sonogram', 'echo'],\n","            'mammography': ['mammography', 'mammogram']\n","        }\n","\n","        self.anatomy_terms = {\n","            'chest': ['chest', 'thorax', 'lung', 'heart', 'rib'],\n","            'abdomen': ['abdomen', 'stomach', 'liver', 'kidney'],\n","            'spine': ['spine', 'vertebra', 'spinal', 'lumbar', 'cervical'],\n","            'brain': ['brain', 'cerebral', 'cranial', 'head'],\n","            'pelvis': ['pelvis', 'hip', 'pelvic'],\n","            'extremity': ['arm', 'leg', 'hand', 'foot', 'bone']\n","        }\n","\n","    def _load_and_process_data(self, caption_csv: str, concept_csv: str, mapping_csv: str):\n","        \"\"\"Load and preprocess all data files\"\"\"\n","        print(\"Loading data files...\")\n","\n","        # Load files\n","        captions_df = pd.read_csv(caption_csv)\n","        concepts_df = pd.read_csv(concept_csv)\n","        mapping_df = pd.read_csv(mapping_csv)\n","\n","        # Standardize column names\n","        captions_df.columns = [col.strip() for col in captions_df.columns]\n","        concepts_df.columns = [col.strip() for col in concepts_df.columns]\n","        mapping_df.columns = [col.strip() for col in mapping_df.columns]\n","\n","        # Rename to standard names\n","        if 'ID' in captions_df.columns and 'Caption' in captions_df.columns:\n","            captions_df = captions_df.rename(columns={\"ID\": \"id\", \"Caption\": \"caption\"})\n","        if 'ID' in concepts_df.columns and 'CUIs' in concepts_df.columns:\n","            concepts_df = concepts_df.rename(columns={\"ID\": \"id\", \"CUIs\": \"concepts\"})\n","        if 'CUI' in mapping_df.columns and 'Name' in mapping_df.columns:\n","            mapping_df = mapping_df.rename(columns={\"CUI\": \"cui\", \"Name\": \"name\"})\n","\n","        # Build concept mapping\n","        self.concept_map = dict(zip(mapping_df[\"cui\"], mapping_df[\"name\"]))\n","\n","        # Merge data\n","        merged_df = pd.merge(captions_df, concepts_df, on=\"id\", how=\"inner\")\n","\n","        merged_df[\"image_path\"] = merged_df[\"id\"].apply(\n","            lambda x: os.path.join(self.image_dir, f\"{x}.jpg\")\n","        )\n","\n","\n","        existing_mask = merged_df[\"image_path\"].apply(os.path.exists)\n","        merged_df = merged_df[existing_mask].reset_index(drop=True)\n","\n","        merged_df[\"caption\"] = merged_df[\"caption\"].apply(self._clean_caption)\n","\n","\n","        merged_df = merged_df[merged_df[\"caption\"].str.strip() != \"\"].reset_index(drop=True)\n","\n","        word_counts = merged_df[\"caption\"].apply(lambda x: len(x.split()))\n","        merged_df = merged_df[\n","            (word_counts >= 5) & (word_counts <= self.max_caption_length)\n","        ].reset_index(drop=True)\n","\n","        self.df = merged_df\n","        print(f\"Loaded {len(self.df)} valid samples\")\n","\n","    def _clean_caption(self, caption: str) -> str:\n","        \"\"\"Clean and normalize captions\"\"\"\n","        if pd.isna(caption) or caption is None:\n","            return \"\"\n","\n","\n","        caption = re.sub(r'\\s+', ' ', str(caption).strip())\n","\n","        abbreviations = {\n","            r'\\bCT\\b': 'CT',\n","            r'\\bMRI\\b': 'MRI',\n","            r'\\bXR\\b': 'X-ray',\n","            r'\\bmm\\b': 'millimeters',\n","            r'\\bcm\\b': 'centimeters'\n","        }\n","\n","        for abbrev, full in abbreviations.items():\n","            caption = re.sub(abbrev, full, caption, flags=re.IGNORECASE)\n","\n","        return caption\n","\n","    def _get_concept_names(self, concept_string: str) -> List[str]:\n","        \"\"\"Convert CUI codes to readable concept names\"\"\"\n","        if pd.isna(concept_string) or concept_string == '':\n","            return []\n","\n","        cui_codes = str(concept_string).split(';')\n","        concept_names = []\n","\n","        for cui in cui_codes:\n","            cui = cui.strip()\n","            if cui in self.concept_map:\n","                name = self.concept_map[cui].strip()\n","                if name and name not in concept_names:\n","                    concept_names.append(name)\n","\n","        return concept_names\n","\n","    def _create_smart_prompt(self, concept_names: List[str]) -> str:\n","        \"\"\"Create intelligent prompts based on medical concepts\"\"\"\n","        if not concept_names:\n","            return \"medical image:\"\n","\n","        modality = \"medical image\"\n","        concept_text = ' '.join(concept_names).lower()\n","\n","        for mod, terms in self.modality_terms.items():\n","            if any(term in concept_text for term in terms):\n","                modality = mod\n","                break\n","\n","        anatomy = \"\"\n","        for anat, terms in self.anatomy_terms.items():\n","            if any(term in concept_text for term in terms):\n","                anatomy = anat\n","                break\n","\n","        # Create simple, effective prompts\n","        if anatomy and modality != \"medical image\":\n","            return f\"{modality} of {anatomy}:\"\n","        elif modality != \"medical image\":\n","            return f\"{modality}:\"\n","        elif anatomy:\n","            return f\"{anatomy} image:\"\n","        else:\n","            return \"medical image:\"\n","\n","    def _create_concept_aware_prompt(self, concept_names: List[str]) -> str:\n","        \"\"\"Create prompts that incorporate medical concepts directly\"\"\"\n","        if not concept_names:\n","            return \"medical image:\"\n","\n","        clean_concepts = []\n","        for concept in concept_names[:3]:\n","            if concept and len(concept.strip()) > 2:\n","                clean_concepts.append(concept.strip().lower())\n","\n","        if not clean_concepts:\n","            return \"medical image:\"\n","\n","        concept_text = ', '.join(clean_concepts)\n","\n","        modality_found = False\n","        anatomy_found = False\n","\n","        for mod, terms in self.modality_terms.items():\n","            if any(term in concept_text for term in terms):\n","                modality_found = mod\n","                break\n","\n","        for anat, terms in self.anatomy_terms.items():\n","            if any(term in concept_text for term in terms):\n","                anatomy_found = anat\n","                break\n","\n","        # Strategy 1: Include concepts directly in prompt\n","        if modality_found and anatomy_found:\n","            return f\"{modality_found} of {anatomy_found} showing {concept_text}:\"\n","        elif modality_found:\n","            return f\"{modality_found} showing {concept_text}:\"\n","        elif anatomy_found:\n","            return f\"{anatomy_found} image showing {concept_text}:\"\n","        else:\n","            return f\"medical image showing {concept_text}:\"\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Load image\n","        try:\n","            image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n","        except Exception as e:\n","            print(f\"Error loading image {row['image_path']}: {e}\")\n","            # Return a blank image as fallback\n","            image = Image.new('RGB', (224, 224), color='white')\n","\n","\n","        concept_names = self._get_concept_names(row[\"concepts\"])\n","\n","        basic_prompt = self._create_smart_prompt(concept_names)\n","        concept_prompt = self._create_concept_aware_prompt(concept_names)\n","\n","        # Get caption\n","        caption = str(row[\"caption\"])\n","\n","        return {\n","            'image': image,\n","            'prompt': basic_prompt,\n","            'concept_prompt': concept_prompt,\n","            'caption': caption,\n","            'concepts': concept_names,\n","            'image_id': row[\"id\"]\n","        }\n","\n","def collate_fn(batch):\n","    \"\"\"Custom collate function for DataLoader\"\"\"\n","    return {\n","        'images': [item['image'] for item in batch],\n","        'prompts': [item['prompt'] for item in batch],\n","        'concept_prompts': [item['concept_prompt'] for item in batch],\n","        'captions': [item['caption'] for item in batch],\n","        'concepts': [item['concepts'] for item in batch],\n","        'image_ids': [item['image_id'] for item in batch]\n","    }\n","\n","class MedicalCaptionTrainer:\n","    \"\"\"Main trainer class for medical image captioning - COMPLETELY FIXED VERSION\"\"\"\n","\n","    def __init__(self, model_name: str = \"Salesforce/blip-image-captioning-base\"):\n","        self.model_name = model_name\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Load model and processor\n","        print(f\"Loading model: {model_name}\")\n","        self.processor = BlipProcessor.from_pretrained(model_name)\n","        self.model = BlipForConditionalGeneration.from_pretrained(model_name)\n","        self.model.to(self.device)\n","\n","        print(f\"Model loaded on device: {self.device}\")\n","\n","    def generate_caption(self, image, prompt: str = None, concepts: List[str] = None, **generation_kwargs):\n","        \"\"\"Main caption generation method - ROBUST FIX for duplicate parameter error\"\"\"\n","        self.model.eval()\n","\n","        if concepts:\n","\n","            dataset_helper = type('Helper', (), {})()\n","            dataset_helper.modality_terms = {\n","                'radiograph': ['radiograph', 'x-ray', 'xray'],\n","                'ct': ['ct', 'computed tomography', 'cat scan'],\n","                'mri': ['mri', 'magnetic resonance'],\n","                'ultrasound': ['ultrasound', 'sonogram', 'echo'],\n","                'mammography': ['mammography', 'mammogram']\n","            }\n","            dataset_helper.anatomy_terms = {\n","                'chest': ['chest', 'thorax', 'lung', 'heart', 'rib'],\n","                'abdomen': ['abdomen', 'stomach', 'liver', 'kidney'],\n","                'spine': ['spine', 'vertebra', 'spinal', 'lumbar', 'cervical'],\n","                'brain': ['brain', 'cerebral', 'cranial', 'head'],\n","                'pelvis': ['pelvis', 'hip', 'pelvic'],\n","                'extremity': ['arm', 'leg', 'hand', 'foot', 'bone']\n","            }\n","\n","            def create_smart_prompt(concept_names):\n","                if not concept_names:\n","                    return \"medical image:\"\n","\n","                modality = \"medical image\"\n","                concept_text = ' '.join(concept_names).lower()\n","\n","                for mod, terms in dataset_helper.modality_terms.items():\n","                    if any(term in concept_text for term in terms):\n","                        modality = mod\n","                        break\n","\n","                anatomy = \"\"\n","                for anat, terms in dataset_helper.anatomy_terms.items():\n","                    if any(term in concept_text for term in terms):\n","                        anatomy = anat\n","                        break\n","\n","                if anatomy and modality != \"medical image\":\n","                    return f\"{modality} of {anatomy}:\"\n","                elif modality != \"medical image\":\n","                    return f\"{modality}:\"\n","                elif anatomy:\n","                    return f\"{anatomy} image:\"\n","                else:\n","                    return \"medical image:\"\n","\n","            concept_prompt = create_smart_prompt(concepts)\n","        else:\n","            concept_prompt = prompt or \"medical image:\"\n","\n","        approaches = [\n","            {\"prompt\": \"\", \"name\": \"no_prompt\"},\n","            {\"prompt\": concept_prompt, \"name\": \"concept_prompt\"}\n","        ]\n","\n","        if prompt and prompt != concept_prompt:\n","            approaches.insert(1, {\"prompt\": prompt, \"name\": \"custom_prompt\"})\n","\n","        best_caption = \"\"\n","        best_score = -1\n","\n","        for approach in approaches:\n","            try:\n","                current_prompt = approach[\"prompt\"]\n","\n","                if current_prompt == \"\":\n","                    # Image-only processing\n","                    processor_outputs = self.processor(images=image, return_tensors=\"pt\")\n","                else:\n","                    # Image + text processing\n","                    processor_outputs = self.processor(images=image, text=current_prompt, return_tensors=\"pt\")\n","\n","                model_inputs = {}\n","\n","\n","                valid_model_input_keys = {'pixel_values', 'input_ids', 'attention_mask'}\n","\n","                for key, value in processor_outputs.items():\n","                    if key in valid_model_input_keys:\n","                        model_inputs[key] = value.to(self.device)\n","\n","                generation_params = {\n","                    'max_new_tokens': 45,\n","                    'min_new_tokens': 8,\n","                    'num_beams': 4,\n","                    'early_stopping': True,\n","                    'no_repeat_ngram_size': 2,\n","                    'repetition_penalty': 2.0,\n","                    'length_penalty': 0.9,\n","                    'do_sample': False,\n","                }\n","\n","\n","                tokenizer = self.processor.tokenizer\n","\n","                self.model.generation_config.pad_token_id = tokenizer.pad_token_id\n","\n","\n","                for key, value in generation_kwargs.items():\n","                    if key not in ['pad_token_id', 'eos_token_id', 'bos_token_id']:\n","                        generation_params[key] = value\n","\n","                with torch.no_grad():\n","                    outputs = self.model.generate(**model_inputs, **generation_params)\n","\n","                # Decode and clean\n","                caption = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n","                caption = self._clean_generated_caption(caption, current_prompt)\n","\n","                # Score the caption\n","                if concepts:\n","                    score = self._score_caption_with_concepts(caption, concepts)\n","                else:\n","                    score = self._score_caption(caption)\n","\n","                if score > best_score:\n","                    best_score = score\n","                    best_caption = caption\n","\n","            except Exception as e:\n","                print(f\"Generation error with {approach['name']}: {e}\")\n","                continue\n","\n","        # Post-process to enhance concept coverage\n","        if concepts and best_caption:\n","            best_caption = self._enhance_caption_with_concepts(best_caption, concepts)\n","\n","        return best_caption if best_caption else \"Medical image showing pathological findings.\"\n","\n","    def _clean_generated_caption(self, caption: str, prompt: str) -> str:\n","        \"\"\"Clean and post-process generated captions\"\"\"\n","        if not caption:\n","            return \"\"\n","\n","        # Remove the prompt from the beginning\n","        if prompt and caption.lower().startswith(prompt.lower()):\n","            caption = caption[len(prompt):].strip()\n","\n","        # Remove common repetitive patterns\n","        caption = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', caption)\n","        caption = re.sub(r'\\b(\\w+)\\s+\\1\\s+\\1\\b', r'\\1', caption)\n","\n","        # Remove excessive punctuation\n","        caption = re.sub(r'[,]{2,}', ',', caption)\n","        caption = re.sub(r'[:]{2,}', ':', caption)\n","\n","        # Clean up spaces and punctuation\n","        caption = re.sub(r'\\s+', ' ', caption)\n","        caption = caption.strip(' ,:.-')\n","\n","        # Ensure proper capitalization\n","        if caption:\n","            caption = caption[0].upper() + caption[1:] if len(caption) > 1 else caption.upper()\n","\n","        return caption\n","\n","    def _score_caption(self, caption: str) -> float:\n","        \"\"\"Score caption quality (higher is better)\"\"\"\n","        if not caption or len(caption.strip()) < 5:\n","            return -1\n","\n","        words = caption.split()\n","        if len(words) < 3:\n","            return -1\n","\n","        # Penalize repetitive captions\n","        unique_words = set(words)\n","        repetition_ratio = len(unique_words) / len(words)\n","\n","        # Penalize very short or very long captions\n","        length_score = min(len(words) / 20, 1.0)\n","\n","        # Bonus for medical terms\n","        medical_terms = ['radiograph', 'ct', 'mri', 'scan', 'image', 'showing', 'demonstrates', 'findings']\n","        medical_bonus = sum(1 for term in medical_terms if term in caption.lower()) * 0.1\n","\n","        # Overall score\n","        score = repetition_ratio * length_score + medical_bonus\n","        return score\n","\n","    def _score_caption_with_concepts(self, caption: str, concepts: List[str]) -> float:\n","        \"\"\"Score caption quality with concept awareness\"\"\"\n","        if not caption or len(caption.strip()) < 5:\n","            return -1\n","\n","        words = caption.split()\n","        if len(words) < 3:\n","            return -1\n","\n","        # Basic quality score\n","        unique_words = set(words)\n","        repetition_ratio = len(unique_words) / len(words)\n","        length_score = min(len(words) / 15, 1.0)\n","\n","        # Concept coverage bonus\n","        concept_score = 0\n","        if concepts:\n","            mentioned_concepts = 0\n","            for concept in concepts:\n","                if concept.lower() in caption.lower():\n","                    mentioned_concepts += 1\n","            concept_score = mentioned_concepts / len(concepts)\n","\n","        # Medical terminology bonus\n","        medical_terms = ['radiograph', 'ct', 'mri', 'scan', 'image', 'showing', 'demonstrates',\n","                        'findings', 'medical', 'anatomy', 'pathology', 'lesion', 'abnormal']\n","        medical_bonus = sum(1 for term in medical_terms if term in caption.lower()) * 0.05\n","\n","        # Weighted final score\n","        score = (repetition_ratio * 0.4 + length_score * 0.3 + concept_score * 0.2 + medical_bonus * 0.1)\n","        return score\n","\n","    def _enhance_caption_with_concepts(self, caption: str, concepts: List[str]) -> str:\n","        \"\"\"Post-process caption to better incorporate concepts\"\"\"\n","        if not concepts or not caption:\n","            return caption\n","\n","        # Check which concepts are missing\n","        missing_concepts = []\n","        for concept in concepts[:2]:  # Only consider top 2 concepts\n","            if concept.lower() not in caption.lower():\n","                # Simplify concept names for better integration\n","                simplified = self._simplify_concept_name(concept)\n","                if simplified and simplified not in caption.lower():\n","                    missing_concepts.append(simplified)\n","\n","        # If important concepts are missing, try to add them naturally\n","        if missing_concepts and len(caption.split()) < 20:  # Only if caption isn't too long\n","            # Add missing concepts at the end\n","            additional_info = ', '.join(missing_concepts[:1])  # Add max 1 missing concept\n","            if not caption.endswith('.'):\n","                caption += f\" with {additional_info}\"\n","            else:\n","                caption = caption[:-1] + f\" with {additional_info}.\"\n","\n","        return caption\n","\n","    def _simplify_concept_name(self, concept: str) -> str:\n","        \"\"\"Simplify medical concept names for natural integration\"\"\"\n","        if not concept:\n","            return \"\"\n","\n","        # Common simplifications\n","        simplifications = {\n","            'structure of': '',\n","            ', unspecified': '',\n","            'radiograph': 'radiographic findings',\n","            'ct': 'CT findings',\n","            'mri': 'MRI findings',\n","            'magnetic resonance imaging': 'MRI',\n","            'computed tomography': 'CT'\n","        }\n","\n","        simplified = concept.lower()\n","        for old, new in simplifications.items():\n","            simplified = simplified.replace(old, new)\n","\n","        simplified = simplified.strip(' ,-.')\n","\n","        # Only return if it's a meaningful addition\n","        if len(simplified) > 3 and simplified not in ['action', 'finding', 'image']:\n","            return simplified\n","\n","        return \"\"\n","\n","    def _calculate_concept_coverage(self, caption: str, concepts: List[str]) -> float:\n","        \"\"\"Calculate what percentage of concepts are mentioned in caption\"\"\"\n","        if not concepts or not caption:\n","            return 0.0\n","\n","        caption_lower = caption.lower()\n","        mentioned = 0\n","\n","        for concept in concepts:\n","            # Check if concept or parts of it are mentioned\n","            concept_words = concept.lower().split()\n","            if any(word in caption_lower for word in concept_words if len(word) > 3):\n","                mentioned += 1\n","\n","        return mentioned / len(concepts)\n","\n","    def _assess_generation_quality(self, generated: str, ground_truth: str) -> str:\n","        \"\"\"Quick assessment of generation quality\"\"\"\n","        if not generated or len(generated.strip()) < 5:\n","            return \" Too short/empty\"\n","\n","        words = generated.split()\n","        if len(set(words)) < len(words) * 0.7:  # More than 30% repetition\n","            return \"Too repetitive\"\n","\n","        # Check for medical relevance\n","        medical_terms = ['radiograph', 'ct', 'mri', 'scan', 'image', 'showing', 'demonstrates',\n","                        'findings', 'medical', 'clinical', 'anatomy', 'pathology']\n","\n","        has_medical_terms = any(term in generated.lower() for term in medical_terms)\n","\n","        if len(words) >= 5 and has_medical_terms:\n","            return \" Good quality\"\n","        elif len(words) >= 3:\n","            return \" Okay but could be better\"\n","        else:\n","            return \" Poor quality\"\n","\n","    def train_simple(self, train_dataset, val_dataset,\n","                    epochs: int = 5, batch_size: int = 2,\n","                    learning_rate: float = 2e-5,\n","                    save_dir: str = \"./fine_tuned_medical_blip\"):\n","        \"\"\"Simplified training approach - use this if main train() has issues\"\"\"\n","\n","        # Create data loaders with smaller batch size\n","        train_loader = DataLoader(\n","            train_dataset, batch_size=batch_size,\n","            shuffle=True, collate_fn=collate_fn, num_workers=0\n","        )\n","        val_loader = DataLoader(\n","            val_dataset, batch_size=batch_size,\n","            shuffle=False, collate_fn=collate_fn, num_workers=0\n","        )\n","\n","        # Setup optimizer\n","        optimizer = torch.optim.AdamW(\n","            self.model.parameters(),\n","            lr=learning_rate,\n","            weight_decay=0.01\n","        )\n","\n","        self.model.train()\n","        best_val_loss = float('inf')\n","\n","        for epoch in range(epochs):\n","            print(f\"\\n{'='*60}\")\n","            print(f\"Epoch {epoch + 1}/{epochs}\")\n","            print(f\"{'='*60}\")\n","\n","            # Training phase\n","            epoch_loss = 0\n","            num_batches = 0\n","\n","            for batch_idx, batch in enumerate(train_loader):\n","                optimizer.zero_grad()\n","                total_loss = 0\n","\n","                # Process each sample individually to avoid batch issues\n","                for i in range(len(batch['images'])):\n","                    try:\n","                        # Use concept-rich prompts when available for better training\n","                        use_concept_prompt = hasattr(batch, 'concept_prompts') and batch.get('concept_prompts')\n","\n","                        if use_concept_prompt and i < len(batch['concept_prompts']):\n","                            training_prompt = batch['concept_prompts'][i]\n","                        else:\n","                            training_prompt = batch['prompts'][i] if i < len(batch['prompts']) else \"medical image:\"\n","\n","                        # Combine prompt and caption for training\n","                        full_text = f\"{training_prompt} {batch['captions'][i]}\"\n","\n","                        inputs = self.processor(\n","                            images=batch['images'][i],\n","                            text=full_text,\n","                            return_tensors=\"pt\",\n","                            padding=True,\n","                            truncation=True,\n","                            max_length=120\n","                        )\n","                        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n","\n","                        # Use the model's built-in loss calculation\n","                        outputs = self.model(**inputs, labels=inputs['input_ids'])\n","                        loss = outputs.loss\n","                        total_loss += loss\n","\n","                    except Exception as e:\n","                        print(f\"Skipping sample {i} due to error: {e}\")\n","                        continue\n","\n","                if total_loss > 0:\n","                    # Average loss over valid samples\n","                    avg_loss = total_loss / len(batch['images'])\n","                    avg_loss.backward()\n","\n","                    # Gradient clipping\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","\n","                    optimizer.step()\n","\n","                    epoch_loss += avg_loss.item()\n","                    num_batches += 1\n","\n","                # Progress reporting\n","                if batch_idx % 10 == 0:\n","                    current_loss = epoch_loss / max(num_batches, 1)\n","                    print(f\"  Batch {batch_idx}/{len(train_loader)} - Loss: {current_loss:.4f}\")\n","\n","            # Validation\n","            val_loss = self._validate_simple(val_loader)\n","\n","            train_loss = epoch_loss / max(num_batches, 1)\n","            print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","            # Save best model\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                print(f\"New best validation loss: {val_loss:.4f}\")\n","                self._save_model(save_dir)\n","\n","        print(f\"\\nTraining completed! Best validation loss: {best_val_loss:.4f}\")\n","        return best_val_loss\n","\n","    def _validate_simple(self, val_loader):\n","        self.model.eval()\n","        total_loss = 0\n","        num_batches = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch_loss = 0\n","                valid_samples = 0\n","\n","                for i in range(len(batch['images'])):\n","                    try:\n","\n","                        use_concept_prompt = hasattr(batch, 'concept_prompts') and batch.get('concept_prompts')\n","\n","                        if use_concept_prompt and i < len(batch['concept_prompts']):\n","                            training_prompt = batch['concept_prompts'][i]\n","                        else:\n","                            training_prompt = batch['prompts'][i] if i < len(batch['prompts']) else \"medical image:\"\n","\n","                        full_text = f\"{training_prompt} {batch['captions'][i]}\"\n","\n","                        inputs = self.processor(\n","                            images=batch['images'][i],\n","                            text=full_text,\n","                            return_tensors=\"pt\",\n","                            padding=True,\n","                            truncation=True,\n","                            max_length=120\n","                        )\n","                        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n","\n","                        outputs = self.model(**inputs, labels=inputs['input_ids'])\n","                        loss = outputs.loss\n","\n","                        batch_loss += loss.item()\n","                        valid_samples += 1\n","\n","                    except Exception as e:\n","                        continue\n","\n","                if valid_samples > 0:\n","                    avg_loss = batch_loss / valid_samples\n","                    total_loss += avg_loss\n","                    num_batches += 1\n","\n","        return total_loss / max(num_batches, 1)\n","\n","    def train(self, train_dataset, val_dataset,\n","              epochs: int = 5, batch_size: int = 4,\n","              learning_rate: float = 2e-5, warmup_ratio: float = 0.1,\n","              save_dir: str = \"./fine_tuned_medical_blip\"):\n","        # Create data loaders\n","        train_loader = DataLoader(\n","            train_dataset, batch_size=batch_size,\n","            shuffle=True, collate_fn=collate_fn, num_workers=2\n","        )\n","        val_loader = DataLoader(\n","            val_dataset, batch_size=batch_size,\n","            shuffle=False, collate_fn=collate_fn, num_workers=2\n","        )\n","\n","        # Setup optimizer and scheduler\n","        optimizer = torch.optim.AdamW(\n","            self.model.parameters(),\n","            lr=learning_rate,\n","            weight_decay=0.01,\n","            eps=1e-8\n","        )\n","\n","        total_steps = len(train_loader) * epochs\n","        warmup_steps = int(total_steps * warmup_ratio)\n","\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=warmup_steps,\n","            num_training_steps=total_steps\n","        )\n","\n","        # Training loop\n","        self.model.train()\n","        best_val_loss = float('inf')\n","\n","        for epoch in range(epochs):\n","            print(f\"\\n{'='*60}\")\n","            print(f\"Epoch {epoch + 1}/{epochs}\")\n","            print(f\"{'='*60}\")\n","\n","            # Training phase\n","            train_loss = self._train_epoch(train_loader, optimizer, scheduler)\n","\n","            # Validation phase\n","            val_loss = self._validate_epoch(val_loader)\n","\n","            print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","            # Save best model\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                print(f\"New best validation loss: {val_loss:.4f}\")\n","                self._save_model(save_dir)\n","\n","    def _train_epoch(self, train_loader, optimizer, scheduler):\n","        \"\"\"Train for one epoch\"\"\"\n","        self.model.train()\n","        total_loss = 0\n","        num_batches = 0\n","\n","        for batch_idx, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            batch_loss = 0\n","            valid_samples = 0\n","\n","            for i in range(len(batch['images'])):\n","                try:\n","                    # FIXED: Process everything together for BLIP\n","                    full_text = f\"{batch['prompts'][i]} {batch['captions'][i]}\"\n","\n","                    # Process image and full text together\n","                    inputs = self.processor(\n","                        images=batch['images'][i],\n","                        text=full_text,\n","                        return_tensors=\"pt\",\n","                        padding=True,\n","                        truncation=True,\n","                        max_length=150\n","                    )\n","                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n","\n","                    # Create labels: shift input_ids to create target sequence\n","                    input_ids = inputs['input_ids']\n","\n","                    # For BLIP, we need to create proper labels\n","                    prompt_tokens = self.processor(\n","                        text=batch['prompts'][i],\n","                        return_tensors=\"pt\",\n","                        add_special_tokens=False\n","                    )['input_ids']\n","\n","                    # Create labels by masking the prompt part (-100 ignores these tokens)\n","                    labels = input_ids.clone()\n","                    prompt_length = prompt_tokens.shape[1]\n","                    labels[:, :prompt_length] = -100  # Ignore prompt tokens in loss\n","\n","                    # Forward pass with proper labels\n","                    outputs = self.model(\n","                        input_ids=input_ids,\n","                        pixel_values=inputs['pixel_values'],\n","                        labels=labels\n","                    )\n","                    loss = outputs.loss\n","\n","                    batch_loss += loss\n","                    valid_samples += 1\n","\n","                except Exception as e:\n","                    print(f\"Error processing sample {i}: {e}\")\n","                    continue\n","\n","            if valid_samples > 0:\n","                # Average loss and backward pass\n","                batch_loss = batch_loss / valid_samples\n","                batch_loss.backward()\n","\n","                # Gradient clipping\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","\n","                optimizer.step()\n","                scheduler.step()\n","\n","                total_loss += batch_loss.item()\n","                num_batches += 1\n","\n","            # Progress reporting\n","            if batch_idx % 20 == 0 and batch_idx > 0:\n","                avg_loss = total_loss / num_batches\n","                lr = scheduler.get_last_lr()[0]\n","                print(f\"  Batch {batch_idx}/{len(train_loader)} - Loss: {avg_loss:.4f}, LR: {lr:.2e}\")\n","\n","        return total_loss / max(num_batches, 1)\n","\n","    def _validate_epoch(self, val_loader):\n","        \"\"\"Validate for one epoch\"\"\"\n","        self.model.eval()\n","        total_loss = 0\n","        num_batches = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch_loss = 0\n","                valid_samples = 0\n","\n","                for i in range(len(batch['images'])):\n","                    try:\n","                        inputs = self.processor(\n","                            images=batch['images'][i],\n","                            text=batch['prompts'][i],\n","                            return_tensors=\"pt\",\n","                            padding=True,\n","                            truncation=True\n","                        )\n","                        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n","\n","                        labels = self.processor(\n","                            text=batch['captions'][i],\n","                            return_tensors=\"pt\",\n","                            padding=True,\n","                            truncation=True,\n","                            max_length=128\n","                        )\n","                        labels = labels['input_ids'].to(self.device)\n","\n","                        outputs = self.model(**inputs, labels=labels)\n","                        loss = outputs.loss\n","\n","                        batch_loss += loss\n","                        valid_samples += 1\n","\n","                    except Exception as e:\n","                        continue\n","\n","                if valid_samples > 0:\n","                    batch_loss = batch_loss / valid_samples\n","                    total_loss += batch_loss.item()\n","                    num_batches += 1\n","\n","        return total_loss / max(num_batches, 1)\n","\n","    def _save_model(self, save_dir: str):\n","        \"\"\"Save model and processor\"\"\"\n","        os.makedirs(save_dir, exist_ok=True)\n","        self.model.save_pretrained(save_dir)\n","        self.processor.save_pretrained(save_dir)\n","        print(f\"Model saved to {save_dir}\")\n","\n","    def quick_test(self, test_dataset, num_samples: int = 3):\n","        \"\"\"Quick test to verify generation is working properly with concepts\"\"\"\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK TEST - CONCEPT-AWARE GENERATION\")\n","        print(f\"{'='*60}\")\n","\n","        for i in range(min(num_samples, len(test_dataset))):\n","            sample = test_dataset[i]\n","            image = sample['image']\n","            concepts = sample['concepts']\n","            ground_truth = sample['caption']\n","\n","            print(f\"\\nExample {i+1}\")\n","            print(f\"Concepts: {concepts[:3]}{'...' if len(concepts) > 3 else ''}\")\n","            print(f\"Ground Truth: {ground_truth[:80]}{'...' if len(ground_truth) > 80 else ''}\")\n","            print()\n","\n","            # Test 1: No prompt + concepts\n","            try:\n","                caption1 = self.generate_caption(\n","                    image, \"\", concepts=concepts,\n","                    max_new_tokens=35,\n","                    num_beams=3,\n","                    repetition_penalty=2.0,\n","                    no_repeat_ngram_size=2\n","                )\n","                print(f\" Concept-guided (no prompt): '{caption1}'\")\n","                coverage1 = self._calculate_concept_coverage(caption1, concepts)\n","                print(f\"   Concept coverage: {coverage1:.1%}\")\n","            except Exception as e:\n","                print(f\" Concept-guided failed: {e}\")\n","\n","            # Test 2: Smart prompt + concepts\n","            try:\n","                basic_prompt = sample.get('prompt', 'medical image:')\n","                caption2 = self.generate_caption(\n","                    image, basic_prompt, concepts=concepts,\n","                    max_new_tokens=35,\n","                    num_beams=3,\n","                    repetition_penalty=2.0,\n","                    no_repeat_ngram_size=2\n","                )\n","                print(f\" Smart prompt + concepts: '{caption2}'\")\n","                coverage2 = self._calculate_concept_coverage(caption2, concepts)\n","                print(f\"   Concept coverage: {coverage2:.1%}\")\n","            except Exception as e:\n","                print(f\" Smart prompt failed: {e}\")\n","\n","            # Test 3: Concept-rich prompt\n","            try:\n","                concept_prompt = sample.get('concept_prompt', 'medical image:')\n","                caption3 = self.generate_caption(\n","                    image, concept_prompt,\n","                    max_new_tokens=35,\n","                    num_beams=3,\n","                    repetition_penalty=2.0,\n","                    no_repeat_ngram_size=2\n","                )\n","                print(f\"✨ Concept-rich prompt: '{caption3}'\")\n","                coverage3 = self._calculate_concept_coverage(caption3, concepts)\n","                print(f\"   Concept coverage: {coverage3:.1%}\")\n","            except Exception as e:\n","                print(f\"Concept-rich prompt failed: {e}\")\n","\n","            print(\"-\" * 60)\n","\n","    def evaluate_model(self, test_dataset, num_samples: int = None):\n","        \"\"\"Comprehensive evaluation with concept-aware generation\"\"\"\n","        if num_samples is None:\n","            num_samples = min(10, len(test_dataset))\n","\n","        print(f\"\\n{'='*60}\")\n","        print(f\"EVALUATING MODEL ON {num_samples} SAMPLES\")\n","        print(f\"{'='*60}\")\n","\n","        # Test different concept-aware strategies\n","        strategies = [\n","            {\n","                'name': 'Concept-Guided Generation (Best)',\n","                'use_concepts': True,\n","                'use_prompt': False,\n","                'params': {\n","                    'max_new_tokens': 40,\n","                    'num_beams': 4,\n","                    'repetition_penalty': 2.0,\n","                    'no_repeat_ngram_size': 2,\n","                    'length_penalty': 0.9\n","                }\n","            },\n","            {\n","                'name': 'Smart Prompt + Concepts',\n","                'use_concepts': True,\n","                'use_prompt': True,\n","                'params': {\n","                    'max_new_tokens': 40,\n","                    'num_beams': 4,\n","                    'repetition_penalty': 1.8,\n","                    'no_repeat_ngram_size': 2,\n","                    'early_stopping': True\n","                }\n","            },\n","            {\n","                'name': 'Pure Image Captioning (Baseline)',\n","                'use_concepts': False,\n","                'use_prompt': False,\n","                'params': {\n","                    'max_new_tokens': 40,\n","                    'num_beams': 4,\n","                    'repetition_penalty': 2.0,\n","                    'no_repeat_ngram_size': 2,\n","                    'length_penalty': 0.8\n","                }\n","            }\n","        ]\n","\n","        for strategy in strategies:\n","            print(f\"\\n--- {strategy['name']} ---\")\n","\n","            total_coverage = 0\n","            valid_samples = 0\n","\n","            for i in range(min(3, num_samples)):  # Show 3 examples per strategy\n","                sample = test_dataset[i]\n","                image = sample['image']\n","                concepts = sample['concepts']\n","                ground_truth = sample['caption']\n","\n","                # Generate caption based on strategy\n","                if strategy['use_concepts'] and strategy['use_prompt']:\n","                    prompt = sample.get('prompt', 'medical image:')\n","                    generated = self.generate_caption(image, prompt, concepts=concepts, **strategy['params'])\n","                elif strategy['use_concepts']:\n","                    generated = self.generate_caption(image, \"\", concepts=concepts, **strategy['params'])\n","                else:\n","                    generated = self.generate_caption(image, \"\", **strategy['params'])\n","\n","                print(f\"\\nExample {i+1}:\")\n","                print(f\"Concepts: {concepts[:3]}{'...' if len(concepts) > 3 else ''}\")\n","                print(f\"Ground Truth: {ground_truth[:80]}{'...' if len(ground_truth) > 80 else ''}\")\n","                print(f\"Generated: {generated}\")\n","\n","                # Calculate concept coverage\n","                coverage = self._calculate_concept_coverage(generated, concepts)\n","                total_coverage += coverage\n","                valid_samples += 1\n","\n","                # Quality assessment\n","                quality = self._assess_generation_quality(generated, ground_truth)\n","                print(f\"Quality: {quality}\")\n","                print(f\"Concept Coverage: {coverage:.1%}\")\n","                print(\"-\" * 40)\n","\n","            # Strategy summary\n","            avg_coverage = total_coverage / max(valid_samples, 1)\n","\n","\n","\n","def main():\n","    \"\"\"Main execution function\"\"\"\n","\n","    # Configuration\n","    config = {\n","        'image_dir': \"/content/drive/MyDrive/medical/development-3/development/train/train\",  # UPDATE THIS PATH\n","        'caption_csv': \"/content/drive/MyDrive/medical/development-3/development/train/train_captions.csv\",  # UPDATE THIS PATH\n","        'concept_csv': \"/content/drive/MyDrive/medical/development-3/development/train/train_concepts.csv\",  # UPDATE THIS PATH\n","        'mapping_csv': \"/content/drive/MyDrive/medical/cui_names.csv\",  # UPDATE THIS PATH\n","        'model_name': \"Salesforce/blip-image-captioning-base\",\n","        'epochs': 5,\n","        'batch_size': 4,\n","        'learning_rate': 2e-5,\n","        'save_dir': \"/content/drive/MyDrive/medical/best_medical_blip\"\n","    }\n","\n","\n","    # Load dataset\n","    print(\"Loading dataset...\")\n","    full_dataset = MedicalCaptionDataset(\n","        image_dir=config['image_dir'],\n","        caption_csv=config['caption_csv'],\n","        concept_csv=config['concept_csv'],\n","        mapping_csv=config['mapping_csv'],\n","        processor=None  # Will be set by trainer\n","    )\n","\n","    # Initialize trainer\n","    trainer = MedicalCaptionTrainer(model_name=config['model_name'])\n","\n","    # Set processor in dataset\n","    full_dataset.processor = trainer.processor\n","\n","        # Take random 10,000 subset\n","    subset_size = min(10_000, len(full_dataset))\n","    indices = torch.randperm(len(full_dataset))[:subset_size].tolist()\n","    full_dataset = torch.utils.data.Subset(full_dataset, indices)\n","\n","    # Proceed with split\n","    train_size = int(0.8 * len(full_dataset))\n","    val_size = int(0.1 * len(full_dataset))\n","    test_size = len(full_dataset) - train_size - val_size\n","\n","    train_dataset, temp_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size + test_size])\n","    val_dataset, test_dataset = torch.utils.data.random_split(temp_dataset, [val_size, test_size])\n","    print(f\"Dataset split: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n","\n","    # Quick test before training\n","    print(\"\\n BEFORE TRAINING - QUICK TEST:\")\n","    trainer.quick_test(test_dataset, num_samples=3)\n","\n","    # Train model (using simplified approach to avoid batch size issues)\n","    print(\"\\n STARTING TRAINING (Simplified Method):\")\n","    trainer.train_simple(\n","        train_dataset=train_dataset,\n","        val_dataset=val_dataset,\n","        epochs=config['epochs'],\n","        batch_size=2,  # Smaller batch size to avoid memory issues\n","        learning_rate=config['learning_rate'],\n","        save_dir=config['save_dir']\n","    )\n","\n","    # Evaluate after training\n","    print(\"\\nAFTER TRAINING:\")\n","    trainer.evaluate_model(test_dataset, num_samples=5)\n","\n","    print(f\"\\n Training complete! Model saved to {config['save_dir']}\")\n","    print(\"To use the trained model later:\")\n","    print(f\"  model = BlipForConditionalGeneration.from_pretrained('{config['save_dir']}')\")\n","    print(f\"  processor = BlipProcessor.from_pretrained('{config['save_dir']}')\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"C2pN6oSS6IFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11603,"status":"ok","timestamp":1751044057493,"user":{"displayName":"Ivan Mihov","userId":"05068184991104003121"},"user_tz":-180},"id":"geIaujlH7y8D","outputId":"5c7a805c-6e16-4b15-ef48-82dda37e98ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.36.2\n","  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/126.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft==0.7.1\n","  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (0.33.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2.32.3)\n","Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n","  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (2.6.0+cu124)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (1.8.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (1.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.1) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (2025.6.15)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (3.0.2)\n","Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers, peft\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.2\n","    Uninstalling tokenizers-0.21.2:\n","      Successfully uninstalled tokenizers-0.21.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.52.4\n","    Uninstalling transformers-4.52.4:\n","      Successfully uninstalled transformers-4.52.4\n","  Attempting uninstall: peft\n","    Found existing installation: peft 0.15.2\n","    Uninstalling peft-0.15.2:\n","      Successfully uninstalled peft-0.15.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed peft-0.7.1 tokenizers-0.15.2 transformers-4.36.2\n"]}],"source":["# !pip install -U transformers==4.36.2 peft==0.7.1\n","# !pip install sacrebleu rouge-score bert-score datasets evaluate"]},{"cell_type":"markdown","source":["# Enhanced Med-BLIP-2: Concept-Aware Medical Captioning\n","\n","This script fine-tunes `Salesforce/blip2-opt-2.7b` using medical images and concept-guided prompts via LoRA adapters.\n","\n","## Dataset Inputs\n","- Images: JPEGs in `/train/{ID}.jpg`\n","- Captions: `train_captions.csv` (`ID`, `Caption`)\n","- Concepts: `train_concepts.csv` (`ID`, `CUIs`)\n","- Mapping: `cui_names.csv` (`CUI`, `Name` → natural text)\n","\n","## Model\n","- Backbone: `Salesforce/blip2-opt-2.7b`\n","- Adapter: `NouRed/Med-BLIP-2-QLoRA` (PEFT LoRA)\n","- Tokenizer: `AutoProcessor` with `use_fast=False`\n","- Trainable layers: `qformer`, `language_projection`\n","\n","## Key Features\n","- Prompts include mapped medical concepts (UMLS → natural terms)\n","- Dynamic prompting templates for medical context\n","- Concept filtering and length-based data cleaning\n","- Mixed-precision training with gradient accumulation\n","- Repetition control in generation (`no_repeat_ngram_size=3`, `repetition_penalty=1.3`)\n","\n","## Training\n","- Loss: CrossEntropy with label smoothing (`ignore_index=-100`)\n","- Optimizer: AdamW (`lr=2e-5`, `weight_decay=0.01`)\n","- Scheduler: Cosine decay with warmup\n","- Epochs: 5 (configurable)\n","- Gradient clipping: max norm = 1.0\n","- Data split: 90% train / 10% val (random)\n","\n","## Generation\n","- Beam search with 5 beams\n","- Concept-driven prompt per image\n","- Cleaned output (prompt stripped, repetition reduced)\n","\n","##Results\n","- AVG_PRED_LENGTH     : 24.1200\n","- AVG_REF_LENGTH      : 22.3200\n","- BLEU_SCORE          : 1.5318\n","- ROUGE_L             : 0.0949\n","- BERTSCORE_F1        : 0.6185\n","## Output\n","- Fine-tuned model saved at: `cfg[\"save_dir\"]`\n","- Can be reloaded with `from_pretrained()`\n","\n","> Tip: update paths in `cfg = {...}` before training.\n"],"metadata":{"id":"60pWrtKG3al3"}},{"cell_type":"code","source":["# Enhanced Med-BLIP-2 Training Code\n","# Includes full dataset and trainer classes with concept integration, decoding, and tuning improvements\n","\n","import os, gc, json, logging, warnings, re, random\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torchvision.transforms as transforms\n","import pandas as pd\n","from tqdm import tqdm\n","from peft import PeftModel\n","from transformers import (\n","    AutoProcessor, Blip2ForConditionalGeneration,\n","    get_cosine_schedule_with_warmup, set_seed\n",")\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.nn.utils import clip_grad_norm_\n","\n","warnings.filterwarnings('ignore')\n","set_seed(42)\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","class EnhancedMedicalConceptDataset(Dataset):\n","    def __init__(self, image_dir, caption_csv, concept_csv, mapping_csv, max_samples=5000, img_size=224):\n","        caps = pd.read_csv(caption_csv).rename(columns={'ID':'id','Caption':'caption'})\n","        conc = pd.read_csv(concept_csv).rename(columns={'ID':'id','CUIs':'concepts'})\n","        mapping = pd.read_csv(mapping_csv).rename(columns={'CUI':'cui','Name':'name'})\n","        self.concept_map = dict(zip(mapping.cui, mapping.name))\n","\n","        df = caps.merge(conc, on='id').dropna()\n","        df['concept_text'] = df['concepts'].apply(self._map_concepts)\n","        df['image_path'] = df['id'].apply(lambda x: os.path.join(image_dir, f\"{x}.jpg\"))\n","        df = df[df['image_path'].apply(os.path.exists)].copy()\n","        df = self._filter_quality_samples(df)\n","\n","        if max_samples and len(df) > max_samples:\n","            df = df.sample(n=max_samples, random_state=42).copy()\n","\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transforms.Compose([\n","            transforms.Resize((img_size, img_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","        ])\n","\n","    def _filter_quality_samples(self, df):\n","        df = df[df['caption'].str.len().between(20, 500)]\n","        df = df[df['concept_text'].str.len() > 5]\n","        df = df[~df['caption'].str.contains(r'^[A-Za-z0-9_\\-\\.]+\\.(jpg|png|jpeg)$', regex=True)]\n","        return df\n","\n","    def _map_concepts(self, cui_str):\n","        cuis = re.split(r'[;|,\\s]+', str(cui_str))\n","        names = [self.concept_map.get(c.strip(), c.strip()) for c in cuis if c.strip()]\n","        return ', '.join(sorted(set(filter(lambda x: len(x) > 2, names))))\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, i):\n","        row = self.df.iloc[i]\n","        img = Image.open(row.image_path).convert(\"RGB\")\n","        return {\n","            \"image\": img,\n","            \"caption\": row.caption,\n","            \"concept_text\": row.concept_text,\n","            \"image_id\": row.id\n","        }\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        imgs = [transforms.Resize((224,224))(b[\"image\"]) for b in batch]\n","        prompts = [f\"Radiological findings: {b['concept_text']}. What does this image show?\" for b in batch]\n","        return {\n","            \"images\": imgs,\n","            \"prompts\": prompts,\n","            \"captions\": [b[\"caption\"] for b in batch],\n","            \"concepts\": [b[\"concept_text\"] for b in batch],\n","            \"image_ids\": [b[\"image_id\"] for b in batch]\n","        }\n","\n","class ImprovedMedBLIPTrainer:\n","    def __init__(self, base_model=\"Salesforce/blip2-opt-2.7b\", adapter=\"NouRed/Med-BLIP-2-QLoRA\"):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.grad_accum_steps = 16\n","        self._clear_mem()\n","        self._load_model(base_model, adapter)\n","        self.scaler = GradScaler()\n","\n","        self.prompt_templates = [\n","            \"Radiological findings: {concept_text}. What does this image show?\",\n","            \"Based on {concept_text}, describe this medical image.\",\n","            \"Imaging context: {concept_text}. Explain the findings.\",\n","            \"{concept_text}. Describe the anatomical or pathological features.\"\n","        ]\n","\n","    def _clear_mem(self):\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","            torch.cuda.reset_peak_memory_stats()\n","        gc.collect()\n","\n","    def _load_model(self, base_model, adapter):\n","        self.processor = AutoProcessor.from_pretrained(base_model, use_fast=False)\n","        base = Blip2ForConditionalGeneration.from_pretrained(base_model, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n","\n","        try:\n","            self.model = PeftModel.from_pretrained(base, adapter, torch_dtype=torch.float16, trust_remote_code=True)\n","        except:\n","            self.model = base\n","\n","        self.model.train()\n","        for name, param in self.model.named_parameters():\n","            if 'qformer' in name or 'language_projection' in name:\n","                param.requires_grad = True\n","                param.data = param.data.float()\n","            else:\n","                param.requires_grad = False\n","\n","    def augment_prompt(self, concept_text):\n","        if not concept_text.strip():\n","            return \"Describe what is shown in this medical image:\"\n","        return random.choice(self.prompt_templates).format(concept_text=concept_text)\n","\n","    def compute_enhanced_loss(self, outputs, labels):\n","        loss_fct = torch.nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=-100)\n","        shift_logits = outputs.logits[..., :-1, :].contiguous()\n","        shift_labels = labels[..., 1:].contiguous()\n","        return loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","\n","    def train(self, dataset, epochs=5, batch_size=1, lr=2e-5, save_dir=\"./medblip_final\"):\n","        train_len = int(0.9 * len(dataset))\n","        val_len = len(dataset) - train_len\n","        train_ds, val_ds = random_split(dataset, [train_len, val_len])\n","\n","        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n","        val_loader = DataLoader(val_ds, batch_size=1, collate_fn=dataset.collate_fn)\n","\n","        opt = torch.optim.AdamW([p for p in self.model.parameters() if p.requires_grad], lr=lr, weight_decay=0.01)\n","        total_steps = len(train_loader) * epochs // self.grad_accum_steps\n","        sched = get_cosine_schedule_with_warmup(opt, total_steps // 10, total_steps)\n","\n","        for ep in range(epochs):\n","            logger.info(f\"Epoch {ep+1}/{epochs}\")\n","            self._run_epoch(train_loader, opt, sched, training=True)\n","            self._run_epoch(val_loader, opt, sched, training=False)\n","            self._save(save_dir)\n","\n","        return train_ds\n","\n","    def _run_epoch(self, loader, opt, sched, training=True):\n","        self.model.train() if training else self.model.eval()\n","        bar = tqdm(loader, desc=\"Train\" if training else \"Val\")\n","        for i, batch in enumerate(bar):\n","            if training:\n","                prompts = [self.augment_prompt(c) for c in batch[\"concepts\"]]\n","                batch[\"prompts\"] = prompts\n","            inputs = self.processor(images=batch[\"images\"], text=batch[\"prompts\"], return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n","            labels = inputs.input_ids.clone()\n","            with autocast():\n","                outputs = self.model(**inputs, labels=labels)\n","                loss = self.compute_enhanced_loss(outputs, labels)\n","                loss = loss / self.grad_accum_steps\n","            if training:\n","                self.scaler.scale(loss).backward()\n","                if (i+1) % self.grad_accum_steps == 0:\n","                    self.scaler.unscale_(opt)\n","                    clip_grad_norm_(self.model.parameters(), 1.0)\n","                    self.scaler.step(opt)\n","                    self.scaler.update()\n","                    opt.zero_grad()\n","                    sched.step()\n","\n","    def generate_improved(self, image, concept_text=\"\", max_tokens=60):\n","        prompt = self.augment_prompt(concept_text)\n","        inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\").to(self.device)\n","        with torch.no_grad(), autocast():\n","            outputs = self.model.generate(\n","                **inputs, max_new_tokens=max_tokens,\n","                num_beams=5, pad_token_id=self.processor.tokenizer.eos_token_id,\n","                no_repeat_ngram_size=3, repetition_penalty=1.3, length_penalty=1.1,\n","                early_stopping=True\n","            )\n","        return self.processor.batch_decode(outputs, skip_special_tokens=True)[0].replace(prompt, \"\").strip()\n","\n","    def _save(self, save_dir):\n","        os.makedirs(save_dir, exist_ok=True)\n","        self.model.save_pretrained(save_dir)\n","        self.processor.save_pretrained(save_dir)\n","\n","# Training config\n","cfg = {\n","    \"image_dir\": \"/content/drive/MyDrive/medical/development-3/development/train/train\",\n","    \"caption_csv\": \"/content/drive/MyDrive/medical/development-3/development/train/train_captions.csv\",\n","    \"concept_csv\": \"/content/drive/MyDrive/medical/development-3/development/train/train_concepts.csv\",\n","    \"mapping_csv\": \"/content/drive/MyDrive/medical/cui_names.csv\",\n","    \"epochs\": 5,\n","    \"batch_size\": 1,\n","    \"lr\": 2e-5,\n","    \"max_samples\": 2000,\n","    \"save_dir\": \"/content/drive/MyDrive/medical/medblip_improved_final\"\n","}\n","\n","def main():\n","    trainer = ImprovedMedBLIPTrainer()\n","    dataset = EnhancedMedicalConceptDataset(\n","        cfg[\"image_dir\"], cfg[\"caption_csv\"], cfg[\"concept_csv\"],\n","        cfg[\"mapping_csv\"], max_samples=cfg[\"max_samples\"]\n","    )\n","    trainer.train(dataset, cfg[\"epochs\"], cfg[\"batch_size\"], cfg[\"lr\"], cfg[\"save_dir\"])\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"uZDd81f53riF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Evaluation Script"],"metadata":{"id":"uEeeFL9y3zZ6"}},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":844,"referenced_widgets":["8fd44ee1ff0e4e87ab4437c530c39b5d","14525c5b5b0246a18e27a2ff090dd0cf","db822588103e4b23b9092d02eaa84ef4","c08e7d250c1741759b0f7ff20f684e51","71500cdf52d14bfd8fdef49ce2c2af89","b369b0f01c9b4c479445d8c3be4d5299","1f57fabe49454a3c9cd70a71613138eb","6f5ba82a3ab74e01815f4d0f7e4fb948","6609a3d659054a11b9f69faa51779fca","d2efc2435bae471393370058ffd6ffb7","254e0846716b4d3d8d806e44bbbd9dbe"]},"executionInfo":{"elapsed":188700,"status":"error","timestamp":1751054987198,"user":{"displayName":"Ivan Mihov","userId":"05068184991104003121"},"user_tz":-180},"id":"KugmRxERxD_I","outputId":"2977c500-c807-4402-bedb-06a80046dd47"},"outputs":[{"output_type":"stream","name":"stderr","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd44ee1ff0e4e87ab4437c530c39b5d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["📊 Dataset size: 100 samples\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating:   1%|          | 1/100 [00:02<04:17,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Sample 1 ---\n","Concepts: CT, Internal Nare\n","Ground Truth: CT scan of the patient shown obstruction on both side of choanae.\n","Prediction: This image shows the right side of the patient's head. The patient is lying on his back with his head tilted to the left. The patient's head is tilted to the left. The patient's head is tilted to the left. The patient's\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:   2%|▏         | 2/100 [00:05<04:14,  2.60s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Sample 2 ---\n","Concepts: CT, Maxilla bone structure\n","Ground Truth: Coronal CT images showing the left anterior superior alveolar nerve (ASAN) branching inferiorly from the infraorbital nerve. The ASAN pathway is disrupted distally within the anterior maxillary wall due to bony sclerosis.\n","Prediction: This image shows the maxilla bone structure. The maxilla is a bone in the front of the skull that connects the jaw to the rest of the skull. The maxilla is a bone in the front of the skull that connects the jaw to the\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:   3%|▎         | 3/100 [00:05<02:49,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Sample 3 ---\n","Concepts: CT, aortic arch, chest and upper back, enlarged\n","Ground Truth: CT scan of the thorax showing the enlarged mediastinal mass in front of the aortic arch.\n","Prediction: \n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:   4%|▍         | 4/100 [00:06<02:04,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Sample 4 ---\n","Concepts: CT, Posterior part of pelvis, Structure of pelvic region, unspecified\n","Ground Truth: Mean values shown for pelvic incidence (PI) and sacral table angle (STA) in patients with L5 spondylolysis with measures demonstrated on computed tomography from a patient with spondylolysis (PI solid line; STA dashed line).\n","Prediction: What does it tell us about the patient?\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:   5%|▌         | 5/100 [00:09<02:45,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Sample 5 ---\n","Concepts: CT\n","Ground Truth: Image showing the progression of the apparently benign left adrenal incidentaloma (arrow in Figure 1) to adrenocortical carcinoma (arrow). CT scan images nine years later showing the change in the characteristics of the lesion, with a rapid interval growth measuring 57.3 mm (maximum diameter).CT: computed tomography\n","Prediction: The image shows a patient with a large mass in the right lower quadrant of the chest. The mass is located in the right lower quadrant of the chest. The mass is located in the right lower quadrant of the chest.\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating:  84%|████████▍ | 84/100 [02:50<00:32,  2.02s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-16-2461232652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-16-2461232652.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         metrics, detailed_results = evaluator.evaluate_dataset(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_eval_samples\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-16-2461232652.py\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[0;34m(self, dataset, num_samples, save_results)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# Generate prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcept_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-16-2461232652.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, image, concept_text, max_tokens)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             outputs = self.model.generate(\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip_2/modeling_blip_2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage_model_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_model_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m         outputs = self.language_model.generate(\n\u001b[0m\u001b[1;32m   1831\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1795\u001b[0m             )\n\u001b[1;32m   1796\u001b[0m             \u001b[0;31m# 13. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1798\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3179\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3181\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   3182\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3183\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m   1144\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    907\u001b[0m                 )\n\u001b[1;32m    908\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    910\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_layer_norm_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2908\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m         )\n\u001b[0;32m-> 2910\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2911\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m     )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","Standalone evaluation script for trained Medical BLIP-2 model\n","\"\"\"\n","\n","import os, json, logging, warnings, re\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torchvision.transforms as transforms\n","import pandas as pd\n","from tqdm import tqdm\n","from peft import PeftModel\n","from transformers import AutoProcessor, Blip2ForConditionalGeneration\n","from torch.cuda.amp import autocast\n","\n","# Try to import metrics, handle gracefully if missing\n","try:\n","    from datasets import load_metric\n","    METRICS_AVAILABLE = True\n","except ImportError:\n","    METRICS_AVAILABLE = False\n","    print(\"Datasets library not available for metrics\")\n","\n","try:\n","    import evaluate\n","    EVALUATE_AVAILABLE = True\n","except ImportError:\n","    EVALUATE_AVAILABLE = False\n","    print(\" Evaluate library not available\")\n","\n","warnings.filterwarnings('ignore')\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","class MedBLIPEvaluator:\n","    def __init__(self, model_path, base_model=\"Salesforce/blip2-opt-2.7b\"):\n","        \"\"\"\n","        Load trained model for evaluation\n","        Args:\n","            model_path: Path to saved model directory\n","            base_model: Base model name (if needed for processor)\n","        \"\"\"\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model_path = model_path\n","        self._load_trained_model(model_path, base_model)\n","\n","    def _load_trained_model(self, model_path, base_model):\n","        \"\"\"Load the trained model from saved directory\"\"\"\n","        logger.info(f\"🔍 Loading trained model from: {model_path}\")\n","\n","        try:\n","            # Try loading processor from saved model first\n","            self.processor = AutoProcessor.from_pretrained(\n","                model_path,\n","                trust_remote_code=True,\n","                use_fast=False\n","            )\n","            logger.info(\" Loaded processor from saved model\")\n","        except Exception as e:\n","            logger.warning(f\"Could not load processor from saved model: {e}\")\n","            logger.info(f\" Loading processor from base model: {base_model}\")\n","            self.processor = AutoProcessor.from_pretrained(\n","                base_model,\n","                trust_remote_code=True,\n","                use_fast=False\n","            )\n","\n","        try:\n","            # Load the trained model\n","            self.model = Blip2ForConditionalGeneration.from_pretrained(\n","                model_path,\n","                torch_dtype=torch.float16,\n","                device_map=\"auto\",\n","                trust_remote_code=True\n","            )\n","            logger.info(\" Loaded trained model successfully\")\n","        except Exception as e:\n","            logger.error(f\" Failed to load trained model: {e}\")\n","            logger.info(\" Make sure the model was saved correctly and the path is correct\")\n","            raise\n","\n","        self.model.eval()\n","\n","        # Clear memory\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","    def generate(self, image, concept_text=\"\", max_tokens=50):\n","        \"\"\"Generate caption for an image with optional concept guidance\"\"\"\n","        if isinstance(image, str):\n","            image = Image.open(image).convert(\"RGB\")\n","\n","        prompt = f\"Findings: {concept_text}. What does this image show?\" if concept_text else \"What does this image show?\"\n","\n","        inputs = self.processor(\n","            images=image,\n","            text=prompt,\n","            return_tensors=\"pt\"\n","        ).to(self.device)\n","\n","        with torch.no_grad(), autocast():\n","            # Handle pad_token_id\n","            pad_token_id = getattr(self.processor.tokenizer, 'pad_token_id', None)\n","            if pad_token_id is None:\n","                pad_token_id = self.processor.tokenizer.eos_token_id\n","\n","            outputs = self.model.generate(\n","                **inputs,\n","                max_new_tokens=max_tokens,\n","                num_beams=3,\n","                pad_token_id=pad_token_id,\n","                do_sample=False,\n","                early_stopping=True,\n","                repetition_penalty=1.1\n","            )\n","\n","        generated_text = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n","        # Remove the prompt from generated text\n","        return generated_text.replace(prompt, \"\").strip()\n","\n","    def evaluate_dataset(self, dataset, num_samples=None, save_results=True):\n","        \"\"\"Evaluate model on a dataset\"\"\"\n","        logger.info(f\" Starting evaluation on {len(dataset)} samples\")\n","\n","        if num_samples is not None:\n","            num_samples = min(num_samples, len(dataset))\n","            logger.info(f\" Limiting evaluation to {num_samples} samples\")\n","        else:\n","            num_samples = len(dataset)\n","\n","        references = []\n","        predictions = []\n","        detailed_results = []\n","\n","        for i in tqdm(range(num_samples), desc=\"Evaluating\"):\n","            try:\n","                sample = dataset[i]\n","                image = sample[\"image\"]\n","                concept_text = sample.get(\"concept_text\", \"\")\n","                ground_truth = sample[\"caption\"]\n","\n","                # Generate prediction\n","                prediction = self.generate(image, concept_text)\n","\n","                references.append(ground_truth)\n","                predictions.append(prediction)\n","\n","                detailed_results.append({\n","                    \"sample_id\": i,\n","                    \"ground_truth\": ground_truth,\n","                    \"prediction\": prediction,\n","                    \"concept_text\": concept_text\n","                })\n","\n","                # Print a few examples\n","                if i < 5:\n","                    print(f\"\\n--- Sample {i+1} ---\")\n","                    print(f\"Concepts: {concept_text}\")\n","                    print(f\"Ground Truth: {ground_truth}\")\n","                    print(f\"Prediction: {prediction}\")\n","\n","            except Exception as e:\n","                logger.warning(f\"Error processing sample {i}: {e}\")\n","                continue\n","\n","        # Calculate metrics\n","        metrics = self._calculate_metrics(references, predictions)\n","\n","        # Save results if requested\n","        if save_results:\n","            self._save_evaluation_results(detailed_results, metrics)\n","\n","        return metrics, detailed_results\n","\n","    def _calculate_metrics(self, references, predictions):\n","        \"\"\"Calculate evaluation metrics\"\"\"\n","        metrics = {}\n","\n","        # Basic metrics\n","        metrics['num_samples'] = len(predictions)\n","        metrics['avg_pred_length'] = sum(len(p.split()) for p in predictions) / len(predictions)\n","        metrics['avg_ref_length'] = sum(len(r.split()) for r in references) / len(references)\n","\n","        # Try to calculate BLEU, ROUGE, and BERTScore\n","        if METRICS_AVAILABLE:\n","            try:\n","                # BLEU Score\n","                bleu = load_metric(\"sacrebleu\")\n","                bleu_result = bleu.compute(\n","                    predictions=predictions,\n","                    references=[[ref] for ref in references]\n","                )\n","                metrics['bleu_score'] = bleu_result['score']\n","                logger.info(f\" BLEU Score: {bleu_result['score']:.2f}\")\n","            except Exception as e:\n","                logger.warning(f\"Could not calculate BLEU: {e}\")\n","\n","            try:\n","                # ROUGE Score\n","                rouge = load_metric(\"rouge\")\n","                rouge_result = rouge.compute(\n","                    predictions=predictions,\n","                    references=references\n","                )\n","                metrics['rouge_l'] = rouge_result['rougeL'].mid.fmeasure\n","                logger.info(f\" ROUGE-L: {rouge_result['rougeL'].mid.fmeasure:.4f}\")\n","            except Exception as e:\n","                logger.warning(f\"Could not calculate ROUGE: {e}\")\n","\n","            try:\n","                # BERTScore\n","                bertscore = load_metric(\"bertscore\")\n","                bert_result = bertscore.compute(\n","                    predictions=predictions,\n","                    references=references,\n","                    lang=\"en\"\n","                )\n","                metrics['bertscore_f1'] = sum(bert_result['f1']) / len(bert_result['f1'])\n","                logger.info(f\"BERTScore F1: {metrics['bertscore_f1']:.4f}\")\n","            except Exception as e:\n","                logger.warning(f\"Could not calculate BERTScore: {e}\")\n","\n","        elif EVALUATE_AVAILABLE:\n","            try:\n","                # Use evaluate library as fallback\n","                bleu = evaluate.load(\"sacrebleu\")\n","                bleu_result = bleu.compute(\n","                    predictions=predictions,\n","                    references=[[ref] for ref in references]\n","                )\n","                metrics['bleu_score'] = bleu_result['score']\n","                logger.info(f\"✅ BLEU Score: {bleu_result['score']:.2f}\")\n","            except Exception as e:\n","                logger.warning(f\"Could not calculate BLEU with evaluate: {e}\")\n","\n","        else:\n","            logger.warning(\"⚠️ No metrics libraries available. Install with:\")\n","            logger.warning(\"pip install datasets evaluate sacrebleu rouge-score bert-score\")\n","\n","        return metrics\n","\n","    def _save_evaluation_results(self, detailed_results, metrics):\n","        \"\"\"Save evaluation results to files\"\"\"\n","        # Create results directory\n","        results_dir = os.path.join(self.model_path, \"evaluation_results\")\n","        os.makedirs(results_dir, exist_ok=True)\n","\n","        # Save detailed results\n","        detailed_path = os.path.join(results_dir, \"detailed_results.json\")\n","        with open(detailed_path, 'w') as f:\n","            json.dump(detailed_results, f, indent=2)\n","\n","        # Save metrics summary\n","        metrics_path = os.path.join(results_dir, \"metrics_summary.json\")\n","        with open(metrics_path, 'w') as f:\n","            json.dump(metrics, f, indent=2)\n","\n","        logger.info(f\"💾 Saved results to {results_dir}\")\n","\n","        # Print summary\n","        print(f\"\\n{'='*50}\")\n","        print(\"📊 EVALUATION SUMMARY\")\n","        print(f\"{'='*50}\")\n","        for key, value in metrics.items():\n","            if isinstance(value, float):\n","                print(f\"{key.upper():20}: {value:.4f}\")\n","            else:\n","                print(f\"{key.upper():20}: {value}\")\n","        print(f\"{'='*50}\")\n","\n","class MedicalConceptDataset(Dataset):\n","    \"\"\"Same dataset class as in training script\"\"\"\n","    def __init__(self, image_dir, caption_csv, concept_csv, mapping_csv, max_samples=None):\n","        caps = pd.read_csv(caption_csv).rename(columns={'ID':'id','Caption':'caption'})\n","        conc = pd.read_csv(concept_csv).rename(columns={'ID':'id','CUIs':'concepts'})\n","        mapping = pd.read_csv(mapping_csv).rename(columns={'CUI':'cui','Name':'name'})\n","        self.concept_map = dict(zip(mapping.cui, mapping.name))\n","\n","        df = caps.merge(conc, on='id').dropna()\n","        df['concept_text'] = df['concepts'].apply(self._map_concepts)\n","        df['image_path'] = df['id'].apply(lambda x: os.path.join(image_dir, f\"{x}.jpg\"))\n","        df = df[df['image_path'].apply(os.path.exists)].copy()\n","\n","        if max_samples and len(df) > max_samples:\n","            df = df.sample(n=max_samples, random_state=42).copy()\n","\n","        self.df = df.reset_index(drop=True)\n","        print(f\"📊 Dataset size: {len(self.df)} samples\")\n","\n","    def _map_concepts(self, cui_str):\n","        cuis = re.split(r'[;|,\\s]+', str(cui_str))\n","        names = [self.concept_map.get(cui.strip(), cui.strip()) for cui in cuis if cui.strip()]\n","        return ', '.join(sorted(set(names)))\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, i):\n","        row = self.df.iloc[i]\n","        img = Image.open(row.image_path).convert(\"RGB\")\n","        return {\n","            \"image\": img,\n","            \"caption\": row.caption,\n","            \"concept_text\": row.concept_text\n","        }\n","\n","def main():\n","    \"\"\"Main evaluation function\"\"\"\n","\n","    # Configuration - UPDATE THESE PATHS\n","    config = {\n","        \"model_path\": \"/content/drive/MyDrive/medical/best_medical_blip_fixed\",  # Path to your saved model\n","        \"image_dir\": \"/content/drive/MyDrive/medical/development-3/development/train/train\",\n","        \"caption_csv\": \"/content/drive/MyDrive/medical/development-3/development/train/train_captions.csv\",\n","        \"concept_csv\": \"/content/drive/MyDrive/medical/development-3/development/train/train_concepts.csv\",\n","        \"mapping_csv\": \"/content/drive/MyDrive/medical/cui_names.csv\",\n","        \"num_eval_samples\": 100,  # Set to None to evaluate all samples\n","        \"base_model\": \"Salesforce/blip2-opt-2.7b\"\n","    }\n","\n","    try:\n","        # Initialize evaluator\n","        evaluator = MedBLIPEvaluator(\n","            model_path=config[\"model_path\"],\n","            base_model=config[\"base_model\"]\n","        )\n","\n","        # Load dataset (you can also create a separate test set)\n","        dataset = MedicalConceptDataset(\n","            config[\"image_dir\"],\n","            config[\"caption_csv\"],\n","            config[\"concept_csv\"],\n","            config[\"mapping_csv\"],\n","            max_samples=config[\"num_eval_samples\"]\n","        )\n","\n","        # Run evaluation\n","        metrics, detailed_results = evaluator.evaluate_dataset(\n","            dataset,\n","            num_samples=config[\"num_eval_samples\"],\n","            save_results=True\n","        )\n","\n","        print(\" Evaluation completed!\")\n","\n","        # Optional: Test single image\n","        if len(dataset) > 0:\n","            print(f\"\\n Testing single image generation...\")\n","            sample = dataset[0]\n","            result = evaluator.generate(sample[\"image\"], sample[\"concept_text\"])\n","            print(f\"Concept: {sample['concept_text']}\")\n","            print(f\"Generated: {result}\")\n","\n","    except Exception as e:\n","        logger.error(f\" Evaluation failed: {e}\")\n","        raise\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"1ph6Ry_5CP5UGg7EeKGRqpzbR1qrArBBY","authorship_tag":"ABX9TyPjmqz+pDTVv7F+/1cVhKd/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8fd44ee1ff0e4e87ab4437c530c39b5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14525c5b5b0246a18e27a2ff090dd0cf","IPY_MODEL_db822588103e4b23b9092d02eaa84ef4","IPY_MODEL_c08e7d250c1741759b0f7ff20f684e51"],"layout":"IPY_MODEL_71500cdf52d14bfd8fdef49ce2c2af89"}},"14525c5b5b0246a18e27a2ff090dd0cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b369b0f01c9b4c479445d8c3be4d5299","placeholder":"​","style":"IPY_MODEL_1f57fabe49454a3c9cd70a71613138eb","value":"Loading checkpoint shards: 100%"}},"db822588103e4b23b9092d02eaa84ef4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f5ba82a3ab74e01815f4d0f7e4fb948","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6609a3d659054a11b9f69faa51779fca","value":2}},"c08e7d250c1741759b0f7ff20f684e51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2efc2435bae471393370058ffd6ffb7","placeholder":"​","style":"IPY_MODEL_254e0846716b4d3d8d806e44bbbd9dbe","value":" 2/2 [00:04&lt;00:00,  2.12s/it]"}},"71500cdf52d14bfd8fdef49ce2c2af89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b369b0f01c9b4c479445d8c3be4d5299":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f57fabe49454a3c9cd70a71613138eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f5ba82a3ab74e01815f4d0f7e4fb948":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6609a3d659054a11b9f69faa51779fca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2efc2435bae471393370058ffd6ffb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"254e0846716b4d3d8d806e44bbbd9dbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}